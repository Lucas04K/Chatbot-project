Day 28, 04.02.2026  
Protocol writer: Nicole

---

## __Introduction__  
After a lengthy discussion about the exercises on linear regression, we began the 'Understanding Clustering' lecture. We learnt about two methods: k-means and DBSCAN. Afterwards, we broke into our defined groups and completed the exercises.


_By the end of this session, we should be able to:_

* Understand what clustering means and why it’s useful.
* Recognize unsupervised learning vs supervised learning.
* Explain how clustering algorithms group similar data points.
* Interpret examples of clustering results.
* Connect clustering to business and data analytics applications.

---

## __Schedule__
| Time | Notes |
|---|---|
| 09:00 - 11:30 | Selfstudy Career Campus|
| 11:30 - 12:00 | Protocol discussion with Soha|
| 12:00 - 13:00 | Lunch break|
| 13:00 - 14:00 | Exercise  Discussion, Linear Regression|
| 14:00 - 15:20 | Introduction to Clustering with KMeans |
| 15:20 - 16:00 | Youtube  and Introduction to Clustering with ABScan |
| 16:00 onwards | Group work on exercises |
---

## Overview: Understand what clustering means
Clustering is an unsupervised machine learning technique just with numerical patterns.
That means the algorithm finds patterns on its own, rather than learning from pre-labeled examples.

It’s like giving your data a “sorting instinct.” 
The goal:

<ins>Group data points so that:<ins>
* Points within a cluster are similar to each other.
* Points across clusters are different from each other.
Clustering helps uncover hidden structures in data — revealing natural groupings that humans might miss. It enables the identification of inherent groupings within data based on similarities among the data points

## How Clustering Works

Clustering algorithms use distance or similarity to decide which points belong together.
If two cookies have very similar sweetness, shape, and size → they go in the same cluster.

If one is completely different (say, a brownie) → it goes into another cluster.
It’s like giving your data a “sorting instinct.

# Common Clustering Methods
## - K-means
## - DBScan

DBSCAN vs KMeans : - DBSCAN does not need number of clusters -DBSCAN finds outliers automatically -KMeans works best with round, clean clusters

## 1) K-means Clustering Algorithm

K-means is a popular clustering algorithm known for its simplicity and efficiency. Divides data into k groups where each data point belongs to the nearest “center.”  
Find more information about K-means in this [link](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/) 

→ First you have to be sure that you did the installation in your kernel of the needed package: 
__pip install scikit-learn__  
You will need to import: 
- import numpy as np
- import pandas as pd
- from sklearn.preprocessing import StandardScaler
- from sklearn.cluster import KMeans
- import matplotlib.pyplot as plt
- import seaborn as sns

Suppress FutureWarnings, a indicator that certain features or behaviors in the code are planned to change in the future.
* import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)


## Step 1: Read and understand data
- __df = pd.read_csv()__
- __df.head()__
- __df.describe()__
- __df.info()__

## Step 2: Work with the numerical data and filter out the wanted columns
__e.g.__  
* df_num = df[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']]
df_num.head()
  
See if the variables in the data set have large differences between their ranges, plot and descibe it:
- df_num.describe()
- df_num.hist();
- Interpret the plot →  Do you see any large difference? If yes which features?

## Step 3: Scalar by Normalization or Standardisation
Scaling is essential when data features have different scales, magnitudes, or units.

→  Normalization: X_new = (X - X_min)/(X_max - X_min)  

* scaler = MinMaxScaler()
scaler.fit(df_num)
df_num_scaled = scaler.transform(df_num)

→  Standardization: X_new = (X - mean)/Std

* scaler = StandardScaler()
scaler.fit(df_num)
df_num_scaled = scaler.transform(df_num)
df_num_scaled<ins>

## Normalization vs. Standardization: Key Differences
Normalization and standardization are the primary scaling techniques. While often confused, standardization is generally more effective at handling outliers because it explicitly uses the mean and standard deviation.
* Top Techniques: Normalization & Standardization.
* Key Difference: Standardization is more robust against outliers.
* Why: It relies on mean and standard deviation, whereas normalization is more sensitive to extreme values.

## Step 4: Convert Array to DataFrame and check again
The standardized data of the scaler is an array. Convert the array to a pandas dataframe:  
__e.g.__
* df_penguins = pd.DataFrame(df_num_scaled, columns=df_num.columns)
df_penguins.head()
* .describe()
* s.hist();

## Step 5: Labels and Check of them
Since we are not sure about how many clusters we should have, we may need to generate the same labels again and again. Therefore using 'random state' is a good idea. Labels are 0,1,2,3  
__e.g.__
* kmeans = KMeans(n_clusters=2,random_state=42)
kmeans.fit(df_penguins)

2 = The number of clusters to form as well as the number of centroids to generate  
42 = Determines random number generation for centroid initialization.

Retrieve array of cluster labels from our kmeans model
* clusters = kmeans.labels_
clusters

Retrieve unique clusters
* labels = np.unique(clusters)
labels

How many units are in the cluster
* pd.Series(clusters).value_counts().sort_index()

## Step 6: Adding cluster to DataFrame and check
__e.g.__
* df_clustered = df_penguins.copy() 
df_clustered["cluster"] = clusters
df_clustered
* df_clustered.info()
Which label got the cluster
* df_clustered.unique()

## Step 7: Use __elbow method__  and __Inertia__ 

The elbow method is not the only method but in our example we use this. To see how many clusters are recommended for this dataset using the elbow method is one of the most well-known methods in machine learning and could be also used for finding the optimal number of clusters.  

Checking the inertia  
Clustering inertia measures the sum of squared distances between each point and its nearest cluster centroid, reflecting the compactness of clusters.  
__e.g.__  
We have to give a range:
* K = range(1, 15) 
inertia_list = []

* for k in K:
    kmeans = KMeans(n_clusters=k, random_state=1234) # a different random state
    kmeans.fit(df_penguins)
    inertia_list.append(kmeans.inertia_) 
x = list(range(1, len(inertia_list) + 1))

Create a Seaborn line plot
* plt.figure(figsize=(10, 5))
sns.lineplot(x=x, y=inertia_list, marker='o')  
plt.xlabel('Count of Clusters')
plt.ylabel('Inertia')  
plt.title('"Elbow Method" showing the optimal k')
plt.grid(False)  
plt.show()

In the plot you will see a light elbow for k = 3 (e.g., just integer numbers)  which fits our knowledge of the dataset.

## Step 8:  Repeat k-means clustering with k = 3 and plot
__e.g.__
* kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(df_penguins)

* clusters = kmeans.labels_
df_clustered_3 = df_penguins.copy() 
df_clustered_3["cluster"] = clusters
df_clustered_3

Plot __e.g.__:
* sns.scatterplot(data= df_clustered_3,x="bill_length_mm", y= "flipper_length_mm"  

Plot __e.g.__ with cluster in different colors:
* sns.scatterplot(data= df_clustered_3,x="bill_length_mm", y= "flipper_length_mm", hue="cluster")



## 2) Clustering with DBSCAN Method
Find more information about DBScan in this [link](https://www.youtube.com/watch?v=RDZUdRSDOok)

The DBScan method finds clusters based on density — dense regions form clusters, sparse points are treated as outliers.

You will need to import: 
- import numpy as np
- import pandas as pd
- from sklearn.preprocessing import StandardScaler
- from sklearn.cluster import KMeans
- import matplotlib.pyplot as plt
- import seaborn as sns
- from sklearn.neighbors import NearestNeighbors
- from sklearn.cluster import DBSCAN

## Step 1(same as Kmeans): Read and understand data

- __df = pd.read_csv()__
- __df.head()__
- __df.describe()__
- __df.info()__

## Step 2(same as Kmeans): Work with the numerical data and filter out the wanted columns
__e.g.__  
* df_num = df[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']]
df_num.head()
  
See if the variables in the data set have large differences between their ranges, plot and descibe it:
- df_num.describe()
- df_num.hist();
- Interpret the plot →  Do you see any large difference? If yes which features?

## Step 3(same as Kmeans): Scalar by Normalization or Standardisation
Set up the scaler object to standardize the data and using StandardScaler()
You can call the scaled DataFrame "df_num_scaled"

* scaler = StandardScaler()
scaler.fit(df_num)
df_num_scaled = scaler.transform(df_num)
df_num_scaled<ins>

## Step 4(same as Kmeans): Convert Array to DataFrame and check again
The standardized data of the scaler is an array. Convert the array to a pandas dataframe:  
__e.g.__
* df_penguins = pd.DataFrame(df_num_scaled, columns=df_num.columns)
df_penguins.head()
* .describe()
* s.hist();

## Step 5: Labels and Check of them
What do we need as arguments for DBSCAN(eps=?, min_samples=?)  
__min_samples:__
The min_samples might be iteratively refined based on experimental results and insights gained from evaluating clustering performance metrics  
__eps:__
The elbow plot method can be adapted for DBSCAN to help determine an appropriate value for the eps (epsilon) parameter. 

__e.g.__

* X = df_num_scaled  
distances, _ = nbrs.kneighbors(X)
distances

Fit nearest neighbors!
Initializes a NearestNeighbors object with the parameter n_neighbors set to 4. 
This means that for each point in your dataset X, it will find the 4 nearest neighbors.
nbrs = NearestNeighbors(n_neighbors=4).fit(X)

'distances'=  Array where each element distances[i] contains the distances to the nearest neighbors of point X[i].  


## Step 6: Use __elbow method__  and __Inertia__ 

To see how many clusters are recommended for this dataset using the elbow method is one of the most well-known methods in machine learning and could be also used for finding the optimal number of clusters.
Checking the inertia

Clustering inertia measures the sum of squared distances between each point and its nearest cluster centroid, reflecting the compactness of clusters.  
__e.g.__  
We have to give a range:
* K = range(1, 15) 
inertia_list = []

* for k in K:
    kmeans = KMeans(n_clusters=k, random_state=1234) # a different random state
    kmeans.fit(df_penguins)
    inertia_list.append(kmeans.inertia_) 
x = list(range(1, len(inertia_list) + 1))

Create a Seaborn line plot
* plt.figure(figsize=(10, 5))
sns.lineplot(x=x, y=inertia_list, marker='o')  
plt.xlabel('Count of Clusters')
plt.ylabel('Inertia')  
plt.title('"Elbow Method" showing the optimal k')
plt.grid(False)  
plt.show()

In the plot you will see a light elbow for k = 3 (e.g., just integer numbers)  which fits our knowledge of the dataset. The sorted distances are plotted, with the x-axis representing the sorted data points and the y-axis representing the k-distances. Interpreting the Elbow Plot: The "elbow" point in the plot, where the curve bends and the slope changes sharply, indicates a suitable value for eps.

## Step 7:  DBScan Method
__e.g.__
* dbscan = DBSCAN(eps=0.65
                ,min_samples=5)
dbscan.fit(df_num_scaled)

## Step 8: Labels and Check
Write the clusters (dbscan.labels_) to the dataframe as a new column "dbscan_clusters".
* df['dbscan_clusters'] = dbscan.labels_

* df.head()

## Step9: Plot scatterplot
* plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x='bill_length_mm', y='bill_depth_mm', hue
='dbscan_clusters', palette='Set1')
plt.title('DBSCAN Clustering of Penguins')  
plt.xlabel('Bill Length (mm)')  
plt.ylabel('Bill Depth (mm)')  
plt.legend(title='DBSCAN Clusters')  
plt.show()

"Noise" has the label "-1" - these ar apparently the penguins that don't want to be pigeonholed!

### ADDENDUM:  
After applying a clustering algorithm, we analyze:
### Number of clusters
* How many clusters were identified by the model.
### Cluster characteristics
* What values or features characterize each cluster. 
* For example: low, medium, or high values of important variables.
### Differences between clusters
* How clusters differ from each other.
* Which variables are most important in separating them.
### Special cases (if applicable)
* In DBSCAN, we also analyze:
which points are labeled as noise
the density or shape of clusters.
### Overall interpretation
* What the clusters represent in a real-world sense.
* Example: different customer types, price segments, or behavior groups.