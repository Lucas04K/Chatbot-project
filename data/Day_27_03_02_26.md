Day 26, 03.02.2026  
Protocol writer: Soha

---

## __Introduction__  
Regression in Machine Learning
in this lecture we will learn how to predict outcomes using past data and linear regression

By the end of this lesson, you should be able to:
- Understand what linear regression is and why we use it.
- Identify dependent and independent variables.
- Explain slope and intercept in simple terms.
- Evaluate how well a regression model fits data.
- Apply linear regression in Python using basic commands.

---

## __Schedule__
| Time | Notes |
|---|---|
| 09:00 - 09:30 | Protocol discussion with Pretti|
| 09:30 - 10:20 | A/B Testing Exercises Preview|
| 10:20 - 12:00 | Introduction_to_regression Lecture |
| 12:00 - 13:00 | Lunch break|
| 13:00 - 14:00 | Linear Regression in-depth explanation|
| 14:00 onwards | Group work on exercises |
---

## Step 1: Understand Linear Regression
Linear regression finds a straight line that best describes the relationship between two things: y=a+b⋅x

* y → dependent variable (what you want to predict)
* x → independent variable (what affects y)
* a → intercept (value of y when x = 0), a= (start value)
* b → slope (how much y changes when x changes by 1 unit-step), (change rule)

Example: If x = hours studied, y = exam score. The line shows how score changes with study time.

## Step 2: Prepare the Data
After importing the package and loading the dataset >

> Select X (independent features) and Y (dependent target):

* X = data[['your_feature_column']]  # Features
* y = data['your_target_column']     # Target

> Add a constant for the intercept (important for statsmodels):

* X = sm.add_constant(X)

## Step 3: Train the Model

**1. Create the OLS model:**
- model = sm.OLS(y, X)

**2. Fit the model (learn slope and intercept):**
- results = model.fit()

**3. Check the results::**
- results.summary()
- Look for constant → intercept
- Look for feature column coefficient → slope
- Look at R² → how well your line fits
- Look at p-value → if your feature significantly predicts Y

## Step 4: Make Predictions
After the model is trained, you can predict new values:

- new_X = pd.DataFrame({'const': [1], 'your_feature_column': [new_value]})
- predicted_y = results.predict(new_X)
- print(predicted_y)

This predicts the value of y for a new observation of X.

## Step 5: Visualize the Results

**1. Scatter plot of actual points:**
- plt.scatter(data['your_feature_column'], data['your_target_column'], alpha=0.5)

**2. Plot regression line using slope and intercept:**
- plt.plot(data['your_feature_column'], results.params['your_feature_column'] * data['your_feature_column'] + results.params['const'], color='red')
- plt.xlabel("Feature (X)")
- plt.ylabel("Target (y)")
- plt.title("Linear Regression Line")
- plt.show()


## Step 6: Evaluate the Model
**Metrics to understand model performance:**

> Metric | Meaning                                     | Ideal Value         |
| R2     | How much of y variation is explained by X   | Close to 1 = good   | 
| RMSE   | Root mean squared error.                    | Lower = better      | 
| p-value| Tests if the feature significantly affects y|< 0.05 = significant | 


## Step 7: Key Terms
- Feature / X = Input variable (independent)             
- Target / Y = Output variable (dependent)               
- Intercept / a = Y value when X = 0                        
- Slope / b = Change in Y per unit change in X          
- Observation = One row or example in dataset             
- Coefficients = Numbers (a, b) that define the line       
- Residual = Difference between predicted and actual Y 


## Summary
- Linear regression finds the line of best fit between x and y.
- Slope shows how much y changes with X; intercept shows baseline value.
- OLS is the method to find slope and intercept by minimizing squared errors.
- R² and RMSE help evaluate model performance.
- After training, the model can predict new values for y.

## Tips
- Start with one feature (simple regression).
- Visualize data first → helps to understand the relationship.
- Check R² and residuals → see if model makes sense.
- Later, try multiple regression with more than one feature.
- Get data → Train model → Optimize → Predict
- With intercept, slope, and error visually explained


