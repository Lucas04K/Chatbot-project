 Day 26, 02.02.2026  
Protocol writer: Preeti

---

## __Introduction__  
The session was on A/B Testing

---

## __Schedule__
| Time | Notes |
|---|---|
| 09:30 - 10:15 | Discussion on Hypothesis Testing|
| 10:15 - 12.00 | A/B Testing Theory session |
| 12:00 - 13:00 | Lunch break|
| 13:00 - 13:15 | Self study time |
| 13:15 onwards | Group work on exercises |

# A/B Testing Notes

## Overview
A/B Testing (also called randomized controlled experiments) is a method to scientifically evaluate the impact of changes before rolling them out to all users. In simple terms A/B testing is a statistical method used to compare two versions of something (A and B) to find out which performs better. 
* Version A → Control (original)
* Version B → Treatment (changed version)
* Users are randomly divided into two groups
* Results are compared using data

## Purpose of A/B Testing
* To make data-driven decisions
* To measure impact of changes
* To improve business performance
* To find causal effect of an action

## Real-World Examples

### Common A/B Test Scenarios
- **Medical**: Testing new drugs vs. old ones
- **Fitness Apps**: Longer free trial periods → increased sign-ups and conversions?
- **UI/UX**: Button color changes → improved click-through rates?

## Fundamental Steps of AB Testing

### Step 1: Specify Goal and Designs
- Define what you're testing
- Identify the KPI (Key Performance Indicator)
- Design the control vs. treatment experience

### Step 2: Randomly Sample Users for Enrollment
- Select representative user sample
- Ensure groups are comparable

### Step 3: Randomly Assign Users
- **Control variant**: Current state (no changes)
- **Treatment/test variant**: New design/feature
- Random assignment ensures comparability
- Split doesn't have to be 50-50 (e.g., 99%-1% is valid, but requires more time)
- Users shouldn't know which group they're in
- Try to prevent group interaction

### Step 4: Log User Actions and Compute Metrics
- Track relevant user behaviors
- Calculate KPIs for both groups
- Collect sufficient data

### Step 5: Test for Statistical Significance
- Use hypothesis testing
- Determine if differences are due to the change or random chance

---

## Hypothesis Framework

### Formulating Hypotheses
Three key elements:
1. **What will change?** (e.g., tips, revenue, click-through rate)
2. **How will it change?** (e.g., increase, decrease)
3. **What will cause the change?** (e.g., new feature, design change)

### Two Possible Hypotheses

**Null Hypothesis (H₀)** - Default assumption
- There is **no difference** between groups
- Any observed difference is due to random chance

**Alternative Hypothesis (H₁)** - What you're testing
- There **is a difference** between groups
- The change had a real effect


## Statistical Significance: P-Values
### Understanding P-Values
**Common significance thresholds (alpha):**
- 10% (α = 0.10)
- **5% (α = 0.05)** ← Most popular
- 1% (α = 0.01)
### What Does P-Value Mean?
**A p-value of 0.05** means:
- There's a 5% probability of observing the data (or more extreme) **if the null hypothesis is true**
- In other words: Only a 5% chance the difference happened by luck

**A p-value less than 0.05** means:
- Strong evidence against the null hypothesis
- The observed effect is unlikely to be due to random chance

## Final Decision

### Evaluate and Act

**If B is better than A (statistically significant):**
- Roll out to all customers
- Monitor for unexpected effects

**If no significant difference:**
- Keep experimenting
- Try different variations
- Re-evaluate the hypothesis

---
## Key Takeaways

1. **Randomization is crucial** - ensures groups are comparable
2. **Control groups are essential** - provide the counterfactual
3. **Sample size matters** - too small = unreliable results
4. **Statistical significance ≠ practical significance** - p < 0.05 doesn't guarantee business value
5. **Patience required** - rushing experiments leads to poor decisions
6. **One change at a time** - isolate variables to understand causality



