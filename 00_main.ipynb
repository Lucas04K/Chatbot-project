{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e65b5289",
   "metadata": {},
   "source": [
    "## **Lernziele**\n",
    "\n",
    "Durch das Bearbeiten dieser Übungen werden Sie:\n",
    "\n",
    "- Retrieval-Augmented Generation (RAG) und seine Komponenten verstehen.\n",
    "- PDF-Dokumente effektiv laden, vorverarbeiten und verarbeiten.\n",
    "- Textdaten in Embeddings für effiziente Suche umwandeln.\n",
    "- Dokumenten-Retrieval-Systeme mit LangChain und FAISS implementieren und testen.\n",
    "- Retrieval-Systeme mit kostenlosen Sprachmodellen (LLMs) von ChatGroq integrieren.\n",
    "- Ein interaktives chatbasiertes Frage-Antwort-System aufbauen.\n",
    "\n",
    "---\n",
    "\n",
    "## **Übung 1: Setup und Aufwärmen**\n",
    "\n",
    "In dieser Übung richten Sie Ihre Umgebung ein und wählen ein geeignetes Sprachmodell aus.\n",
    "\n",
    "**Schritte:**\n",
    "\n",
    "1. **Umgebungsvariablen laden:** Stellen Sie sicher, dass Ihre Umgebungsvariablen (z.B. API-Schlüssel, Tokens) sicher gespeichert und geladen werden.\n",
    "2. **LLM auswählen:** Wählen Sie ein kostenloses LLM-Modell von ChatGroq aus.\n",
    "3. **Modell instanziieren:** Erstellen Sie eine Instanz Ihres gewählten Modells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c726e326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a023f458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=1024,\n",
    "    timeout=60,\n",
    "    max_retries=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855d3984",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Übung 2: Datenaufnahme**\n",
    "\n",
    "In dieser Übung lernen Sie, PDF-Daten in eine Python-Umgebung zu laden.\n",
    "\n",
    "**Schritte:**\n",
    "\n",
    "1. **PDF-Loader importieren:** Verwenden Sie LangChains `PyPDFLoader`.\n",
    "2. **PDF-Datei laden:** Erstellen Sie eine Funktion zum Lesen der PDF-Datei.\n",
    "3. **PDF-Inhalt anzeigen:** Geben Sie die Seitenanzahl und den Inhalt der ersten Seite aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "083aff05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 Dokumente geladen\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(\"data\")\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    str(data_path),\n",
    "    glob=\"**/*.md\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"},\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"{len(docs)} Dokumente geladen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad71ff25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tag 07, 27.10.2023\n",
      "\n",
      "Logistische Regression\n",
      "\n",
      "---\n",
      "\n",
      "## Grundlegender Überblick\n",
      "\n",
      "- **Überwachtes Lernen**: Nutzt gelabelte Daten zur Modellbildung.\n",
      "- **Klassifikation vs. Regression**:\n",
      "  - Klassifikation ordnet Eingaben diskreten Klassen zu (z.B. Spam/kein Spam).\n",
      "  - Regression sagt kontinuierliche Werte voraus (z.B. Temperatur, Preis).\n",
      "- **Klassifikationsarten**:\n",
      "  - Binär: Zwei Klassen (z.B. traurig/glücklich).\n",
      "  - Multiklassig: Mehr als zwei Klassen (z.B. Katze/Hund/Krokodil).\n",
      "  - Multilabel: E\n",
      "\n",
      "--- Metadaten ---\n",
      "{'source': 'data\\\\protocol_26-01-27.md'}\n"
     ]
    }
   ],
   "source": [
    "if len(docs) > 3:\n",
    "    print(docs[3].page_content[:500])\n",
    "    print(\"\\n--- Metadaten ---\")\n",
    "    print(docs[3].metadata)\n",
    "else:\n",
    "    print(f\"Nur {len(docs)} Dokument(e) vorhanden.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e85a2ec",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Übung 3: Dokument-Chunking**\n",
    "\n",
    "Diese Übung führt das Aufteilen großer Dokumente in handhabbare Textblöcke ein.\n",
    "\n",
    "**Schritte:**\n",
    "\n",
    "1. **Text-Splitter importieren:** Verwenden Sie `RecursiveCharacterTextSplitter`.\n",
    "2. **Dokument aufteilen:** Schreiben Sie eine Funktion, die geladene Dokumente in Chunks aufteilt.\n",
    "3. **Funktion testen:** Überprüfen Sie das Ergebnis, indem Sie die resultierenden Chunks anzeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "645549c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "react_docs = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "869cfc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc99365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def chunk_documents(documents, chunk_size=700, chunk_overlap=150):\n",
    "    \"\"\"\n",
    "    Verbesserte Chunking-Funktion:\n",
    "    - MarkdownHeaderTextSplitter extrahiert Überschriften als Metadaten\n",
    "    - chunk_size=700 statt 200 fuer mehr Kontext pro Chunk\n",
    "    - chunk_overlap=150 fuer bessere Kontinuitaet\n",
    "    - Reichere Metadaten (Quelle, Zeichenanzahl, Header)\n",
    "    \"\"\"\n",
    "\n",
    "    # FIX: Markdown-Überschriften als Metadaten nutzen\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"h1\"),\n",
    "        (\"##\", \"h2\"),\n",
    "        (\"###\", \"h3\"),\n",
    "    ]\n",
    "    md_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on,\n",
    "        strip_headers=False\n",
    "    )\n",
    "\n",
    "    # FIX: Natürliche Trennzeichen priorisieren (kein harter Schnitt mitten im Satz)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    all_chunks = []\n",
    "    for doc in documents:\n",
    "        source_file = doc.metadata.get(\"source\", \"unknown\")\n",
    "\n",
    "        # Erst nach Markdown-Headings aufteilen\n",
    "        md_chunks = md_splitter.split_text(doc.page_content)\n",
    "\n",
    "        # Dann jeden MD-Chunk weiter aufteilen\n",
    "        for md_chunk in md_chunks:\n",
    "            sub_chunks = text_splitter.split_text(md_chunk.page_content)\n",
    "            for sub_chunk in sub_chunks:\n",
    "                days = re.findall(r\"Tag \\d{1,2}\",sub_chunk)\n",
    "                day_info=\"\"\n",
    "                if len(days)>0:\n",
    "                    day_info=days[0]\n",
    "                new_doc = Document(\n",
    "                    page_content=sub_chunk,\n",
    "\n",
    "                    metadata={\n",
    "                        **doc.metadata,\n",
    "                        **md_chunk.metadata,   # h1, h2, h3 aus Überschriften\n",
    "                        \"source_file\": source_file,\n",
    "                        \"char_count\": len(sub_chunk),\n",
    "                        \"day\":day_info,\n",
    "                        \"day index\":day_info,\n",
    "                        \"Was war an\":day_info,\n",
    "                        \"day key\":day_info,\n",
    "                    }\n",
    "\n",
    "                )\n",
    "                all_chunks.append(new_doc)\n",
    "\n",
    "    # FIX: IDs und Gesamtanzahl nach dem Erstellen aller Chunks setzen\n",
    "    total = len(all_chunks)\n",
    "    for i, chunk in enumerate(all_chunks):\n",
    "        chunk.metadata.update({\n",
    "            \"id\": f\"chunk_{i}\",\n",
    "            \"chunk_index\": i,\n",
    "            \"total_chunks\": total,\n",
    "        })\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "react_chunks = chunk_documents(react_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71c0b622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Chunks: 470\n",
      "\n",
      "--- Beispiel-Chunk Inhalt ---\n",
      "# Tag 03, 21.01.2026  \n",
      "Explorative Datenanalyse (kurz EDA)  \n",
      "---\n",
      "\n",
      "--- Beispiel-Chunk Metadaten ---\n",
      "{'source': 'data\\\\protocol_26-01-21.md', 'h1': 'Tag 03, 21.01.2026', 'source_file': 'data\\\\protocol_26-01-21.md', 'char_count': 64, 'id': 'chunk_0', 'chunk_index': 0, 'total_chunks': 470}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Anzahl Chunks: {len(react_chunks)}\")\n",
    "print(\"\\n--- Beispiel-Chunk Inhalt ---\")\n",
    "print(react_chunks[0].page_content)\n",
    "print(\"\\n--- Beispiel-Chunk Metadaten ---\")\n",
    "print(react_chunks[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d497c03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import RecursiveCharacterTextSplitter\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# def chunk_documents(documents, chunk_size=200, chunk_overlap=50):\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(\n",
    "#         chunk_size=chunk_size,\n",
    "#         chunk_overlap=chunk_overlap\n",
    "#     )\n",
    "#     chunks = text_splitter.split_documents(documents=documents)\n",
    "    \n",
    "#     # Just to add id for etch chunks to map it later \n",
    "#     for i, chunk in enumerate(chunks):\n",
    "#          chunk.metadata.update({\n",
    "#         \"id\": f\"chunk_{i}\",\n",
    "#     })\n",
    "    \n",
    "#     return chunks\n",
    "\n",
    "\n",
    "# react_chunks = chunk_documents(react_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4265a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Execute your chunking function and display results here\n",
    "# len(react_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a61a64",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **Übung 4: Embedding und Speicherung**\n",
    "\n",
    "In dieser Übung erstellen Sie Embeddings aus Textblöcken und speichern sie effizient.\n",
    "\n",
    "**Schritte:**\n",
    "\n",
    "1. **Embedding-Modell wählen:** Verwenden Sie `sentence-transformers/all-mpnet-base-v2` von Hugging Face.\n",
    "2. **Embeddings generieren:** Wandeln Sie Dokument-Chunks in Embeddings um.\n",
    "3. **Embeddings speichern:** Speichern Sie diese Embeddings lokal mit FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d59e07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.faiss import DistanceStrategy\n",
    "\n",
    "# Embedding-Modell einmal global instanziieren\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name='sentence-transformers/all-mpnet-base-v2',\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "def embed_and_store(chunks, db_name, embedding):\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embedding,\n",
    "        distance_strategy=DistanceStrategy.COSINE\n",
    "    )\n",
    "    vectorstore.save_local(f\"../vector_databases/vector_db_{db_name}\")\n",
    "    print(f\"Vektordatenbank gespeichert: ../vector_databases/vector_db_{db_name}\")\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08151ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vektordatenbank gespeichert: ../vector_databases/vector_db_chatbot\n"
     ]
    }
   ],
   "source": [
    "# Globales Embedding-Modell übergeben\n",
    "all_embedding = embed_and_store(chunks=react_chunks, db_name=\"chatbot\", embedding=embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ce9bba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Übung 5: Retrieval aus FAISS**\n",
    "\n",
    "Hier lernen Sie, wie man Dokumente aus einer Vektordatenbank mithilfe von Embeddings abruft.\n",
    "\n",
    "**Schritte:**\n",
    "\n",
    "1. **Embeddings laden:** Laden Sie gespeicherte Embeddings aus der FAISS-Datenbank.\n",
    "2. **Retrieval implementieren:** Erstellen Sie eine Logik zum Abrufen relevanter Chunks basierend auf Abfragen.\n",
    "3. **Retriever testen:** Führen Sie die Suche mit Beispielabfragen durch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6775f587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_from_vector_db(vector_db_path, embedding):\n",
    "    react_vectorstore = FAISS.load_local(\n",
    "        folder_path=vector_db_path,\n",
    "        embeddings=embedding,\n",
    "        allow_dangerous_deserialization=True,\n",
    "        distance_strategy=DistanceStrategy.COSINE\n",
    "    )\n",
    "\n",
    "    retriever = react_vectorstore.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\"k\": 6, \"fetch_k\": 20}\n",
    "    )\n",
    "    return retriever, react_vectorstore\n",
    "\n",
    "\n",
    "react_retriever, react_vectorstore = retrieve_from_vector_db(\n",
    "    \"../vector_databases/vector_db_chatbot\",\n",
    "    embedding=embedding_model\n",
    ")\n",
    "\n",
    "print(type(react_retriever), type(react_vectorstore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca40046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your retrieval system with queries\n",
    "react_retriever.get_relevant_documents(\"Anekdote\",k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdc5069",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Übung 6: Retrieval mit LLM verbinden**\n",
    "\n",
    "Nun verbinden Sie das Dokument-Retrieval mit dem Sprachmodell.\n",
    "\n",
    "**Schritte:**\n",
    "\n",
    "1. **Retrieval-Chain erstellen:** Verknüpfen Sie Ihr Retrieval-System mit Ihrem instanziierten LLM.\n",
    "2. **Chain testen:** Bestätigen Sie die Funktionalität, indem Sie Antworten aus abgerufenen Dokumenten generieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6524c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = \"\"\"Du bist ein präziser Assistent, der Fragen ausschließlich auf Basis der bereitgestellten Dokumente beantwortet.\n",
    "\n",
    "Regeln:\n",
    "- Beziehe dich konkret auf den bereitgestellten Kontext\n",
    "- Wenn die Antwort nicht im Kontext enthalten ist, antworte exakt:\n",
    "  \"Diese Information ist in den bereitgestellten Dokumenten nicht enthalten.\"\n",
    "- Nenne am Ende deiner Antwort die verwendete Quelldatei aus den Metadaten, sofern vorhanden\n",
    "\n",
    "Kontext:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1d6376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "def connect_chains(retriever, prompt):\n",
    "    stuff_documents_chain = create_stuff_documents_chain(\n",
    "        llm=llm,\n",
    "        prompt=prompt\n",
    "    )\n",
    "    retrieval_chain = create_retrieval_chain(\n",
    "        retriever=retriever,\n",
    "        combine_docs_chain=stuff_documents_chain\n",
    "    )\n",
    "    return retrieval_chain\n",
    "\n",
    "\n",
    "react_retrieval_chain = connect_chains(react_retriever, prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb73d27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIX: Hilfsfunktion für saubere Ausgabe – vermeidet Wiederholung im Code\n",
    "def ask(question, chain=react_retrieval_chain):\n",
    "    \"\"\"Stellt eine Frage an die RAG-Chain und gibt die Antwort formatiert aus.\"\"\"\n",
    "    result = chain.invoke({\"input\": question})\n",
    "    print(f\"Frage: {question}\")\n",
    "    print(f\"\\nAntwort:\\n{result['answer']}\")\n",
    "    print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d200b6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask(\"Was ist das Takeaway?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6325e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask(\"Fasse Feature Engineering zusammen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b052fd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Übung 7: Interaktives Chat-System**\n",
    "\n",
    "In der letzten Übung bauen Sie ein interaktives chatbasiertes Abfragesystem.\n",
    "\n",
    "**Schritte:**\n",
    "\n",
    "1. **Chat-Oberfläche erstellen:** Entwickeln Sie eine einfache Funktion für interaktive Abfragen.\n",
    "2. **Chat ausführen:** Ermöglichen Sie es Nutzern, Fragen zu stellen und sofortige Antworten zu erhalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c0506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RAG-Chatbot gestartet. Tippe 'quit' oder 'exit' zum Beenden.\\n\")\n",
    "\n",
    "while True:\n",
    "    user_query = input(\"Deine Frage: \").strip()\n",
    "\n",
    "    if not user_query:\n",
    "        continue\n",
    "\n",
    "    if user_query.lower() in (\"quit\", \"exit\", \"q\"):\n",
    "        print(\"Chatbot beendet.\")\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        result = react_retrieval_chain.invoke({\"input\": user_query})\n",
    "        print(f\"\\nAntwort:\\n{result['answer']}\\n\")\n",
    "        print(\"-\" * 60)\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei der Anfrage: {e}\\nBitte versuche es erneut.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638c56d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run and test your interactive chat system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb30ce1f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Fazit & Reflexion**\n",
    "\n",
    "Nach Abschluss dieser Übungen:\n",
    "\n",
    "- Fassen Sie die gelernten Schlüsselkonzepte zusammen.\n",
    "- Reflektieren Sie über die Wirksamkeit und Einschränkungen des kostenlosen LLM und des von Ihnen aufgebauten RAG-Systems.\n",
    "- Überlegen Sie, wie Sie Ihr System in praktischen Anwendungen verbessern oder erweitern könnten.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
